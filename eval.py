#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import argparse
import json
import os
import re
import gc
import torch
from typing import List, Dict, Any, Tuple

from datasets import load_dataset
from vllm import LLM, SamplingParams

# Use same prompts as training (experiments.prompts) so train and eval match
try:
    from experiments.prompts import GUILDER_SYSTEM_PROMPT, SOLVER_SYSTEM_PROMPT
except ImportError:
    GUILDER_SYSTEM_PROMPT = """You are a reasoning planner that creates structured, step-by-step guidelines for solving a given problem.
Your goal is not to answer the question directly, but to produce a high-quality, explicit plan that guides another model to solve it.
Each plan should:
1. Analyze what the problem is asking.
2. Identify the required knowledge, sub-tasks, or reasoning steps.
3. Provide a structured outline or set of instructions to follow.

Format your output as a concise, ordered list of reasoning steps or directives. Avoid giving the final answer."""

    SOLVER_SYSTEM_PROMPT = """You are a careful and disciplined problem solver that follows a given guideline to reason step by step and produce the final answer.

You are provided with:
(1) A QUESTION that needs to be solved.
(2) A GUIDELINE generated by another model that describes how to approach the problem.

Your task:
- STRICTLY follow the provided guideline to perform reasoning.
- You must first think about the reasoning process as an internal monologue before giving the final answer.
- The reasoning process MUST be enclosed within <think> </think> tags.
- The final answer MUST be placed inside \\boxed{}.
- When performing reasoning, use precise and mathematical logic, not vague explanations.
- Do NOT reveal hidden reasoning outside of the <think> block.
- If the guideline has gaps, fill them in coherently without deviating from its intent.

Format:
<think>
Your detailed internal reasoning process, including any calculations, logic, or derivations.
</think>
\\boxed{your final answer here}"""


def write_jsonl(path: str, rows: List[Dict[str, Any]]) -> None:
    with open(path, "w", encoding="utf-8") as f:
        for r in rows:
            f.write(json.dumps(r, ensure_ascii=False) + "\n")


def read_jsonl(path: str) -> List[Dict[str, Any]]:
    data: List[Dict[str, Any]] = []
    with open(path, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if line:
                data.append(json.loads(line))
    return data


def parse_gsm8k_answer(ans: str) -> str:
    if ans is None:
        return ""
    m = re.search(r"####\s*([^\n]+)", ans)
    if m:
        ans = m.group(1)
    ans = ans.strip()
    ans = re.sub(r"[^0-9\-\.,]", "", ans)
    ans = ans.replace(",", "")
    return ans


def parse_boxed_answer(text: str) -> str:
    if text is None:
        return ""
    m = re.search(r"\\boxed\{([^}]*)\}", text)
    if not m:
        return ""
    ans = m.group(1).strip()
    ans = re.sub(r"[^0-9\-\.,]", "", ans)
    ans = ans.replace(",", "")
    return ans


def compute_accuracy(pairs: List[Tuple[str, str]]) -> float:
    if not pairs:
        return 0.0
    correct = 0
    total = len(pairs)
    for gold, pred in pairs:
        if gold == pred and gold != "":
            correct += 1
    return correct / total


def cleanup_model(llm):
    # More aggressive cleanup
    if hasattr(llm, 'llm_engine'):
        del llm.llm_engine
    del llm
    gc.collect()
    torch.cuda.empty_cache()
    torch.cuda.synchronize()  # Wait for all CUDA operations to complete


def main():
    parser = argparse.ArgumentParser()

    parser.add_argument("--guilder_model", type=str, required=True)
    parser.add_argument("--solver_model", type=str, required=True)
    parser.add_argument("--tensor_parallel_size", type=int, default=1)
    parser.add_argument("--gpu_memory_utilization", type=float, default=0.7)  # Lowered from 0.9 to leave room for two models

    parser.add_argument("--dataset", type=str, default="gsm8k",
                       choices=["gsm8k", "math", "commonsenseqa", "strategyqa"],
                       help="Dataset to evaluate on")
    parser.add_argument("--split", type=str, default="test")
    parser.add_argument("--max_samples", type=int, default=None)
    parser.add_argument("--out_dir", type=str, default=None,
                       help="Output directory (default: {dataset}_two_stage_outputs)")

    parser.add_argument("--guilder_max_new_tokens", type=int, default=256)
    parser.add_argument("--guilder_temperature", type=float, default=0.0)
    parser.add_argument("--guilder_top_p", type=float, default=1.0)
    parser.add_argument("--guilder_batch_size", type=int, default=8)

    parser.add_argument("--solver_max_new_tokens", type=int, default=512)
    parser.add_argument("--solver_temperature", type=float, default=0.0)
    parser.add_argument("--solver_top_p", type=float, default=1.0)
    parser.add_argument("--solver_batch_size", type=int, default=4)

    args = parser.parse_args()

    # Set default output directory based on dataset
    if args.out_dir is None:
        args.out_dir = f"./{args.dataset}_two_stage_outputs"
    os.makedirs(args.out_dir, exist_ok=True)
    
    guideline_jsonl = os.path.join(args.out_dir, f"{args.dataset}_{args.split}_guidelines.jsonl")
    pred_jsonl = os.path.join(args.out_dir, f"{args.dataset}_{args.split}_predictions.jsonl")

    print(f"Loading {args.dataset} dataset (split: {args.split})...")
    
    # Load dataset based on name
    if args.dataset == "gsm8k":
        ds = load_dataset("gsm8k", "main", split=args.split)
        question_field = "question"
        answer_field = "answer"
    elif args.dataset == "math":
        ds = load_dataset("HuggingFaceH4/MATH", split=args.split)
        question_field = "problem"
        answer_field = "solution"
    elif args.dataset == "commonsenseqa":
        ds = load_dataset("commonsense_qa", split=args.split)
        question_field = "question"
        answer_field = "answerKey"
        # CommonSenseQA has choices that need to be included in the question
        choices_field = "choices"
    elif args.dataset == "strategyqa":
        # StrategyQA only has train split, so use that
        ds = load_dataset("metaeval/strategyqa", split="train")
        question_field = "question"
        answer_field = "answer"
    else:
        raise ValueError(f"Unknown dataset: {args.dataset}")
    
    if args.max_samples is not None and args.max_samples > 0:
        if args.dataset == "strategyqa":
            # For StrategyQA, randomly sample from train set
            import random
            random.seed(42)  # For reproducibility
            indices = list(range(len(ds)))
            random.shuffle(indices)
            ds = ds.select(indices[:min(args.max_samples, len(ds))])
        else:
            ds = ds.select(range(min(args.max_samples, len(ds))))

    # Format questions (include choices for CommonSenseQA)
    if args.dataset == "commonsenseqa":
        questions = []
        for ex in ds:
            q = ex[question_field]
            choices = ex.get("choices", {})
            if isinstance(choices, dict) and "label" in choices and "text" in choices:
                # Format: "Question\n(A) choice1\n(B) choice2\n..."
                choice_text = "\n".join([f"({label}) {text}" for label, text in zip(choices["label"], choices["text"])])
                q = f"{q}\n\n{choice_text}"
            questions.append(q)
    else:
        questions = [ex[question_field] for ex in ds]
    
    gold_answers = [ex[answer_field] for ex in ds]
    print(f"Loaded {len(questions)} examples.")

    print("========== Stage 1: Guilder ==========")
    guilder_messages = [
        [
            {"role": "system", "content": GUILDER_SYSTEM_PROMPT},
            {"role": "user", "content": q},
        ]
        for q in questions
    ]

    # For merged models, use base model for tokenizer (same tokenizer, avoids config loading issues)
    # vllm will use the model path for weights but can load tokenizer from a different path
    guilder_tokenizer = None
    if "merge_models" in args.guilder_model:
        # Use base model for tokenizer to avoid config.json loading issues
        guilder_tokenizer = "Qwen/Qwen2.5-1.5B-Instruct"
    
    guilder_llm = LLM(
        model=args.guilder_model,
        tokenizer=guilder_tokenizer,
        tensor_parallel_size=args.tensor_parallel_size,
        trust_remote_code=True,
        gpu_memory_utilization=args.gpu_memory_utilization,
        enforce_eager=True,  # Disable CUDA graphs to avoid compilation errors
        max_num_seqs=128,  # Limit concurrent sequences to reduce memory
        max_model_len=8192,  # Reduce max sequence length to save memory
    )

    guilder_params = SamplingParams(
        temperature=args.guilder_temperature,
        top_p=args.guilder_top_p,
        max_tokens=args.guilder_max_new_tokens,
    )

    print("[Guilder] Generating guidelines...")
    all_guidelines: List[str] = []
    for start in range(0, len(guilder_messages), args.guilder_batch_size):
        end = min(start + args.guilder_batch_size, len(guilder_messages))
        batch = guilder_messages[start:end]
        outputs = guilder_llm.chat(messages=batch, sampling_params=guilder_params, use_tqdm=False)
        for out in outputs:
            all_guidelines.append(out.outputs[0].text.strip())

    guideline_rows = [
        {"index": idx, "question": q, "guideline": g}
        for idx, (q, g) in enumerate(zip(questions, all_guidelines))
    ]
    write_jsonl(guideline_jsonl, guideline_rows)
    print(f"[Guilder] Saved guidelines to {guideline_jsonl}")

    print("[Guilder] Cleaning up...")
    cleanup_model(guilder_llm)
    # Additional cleanup to ensure memory is freed
    import time
    time.sleep(2)  # Give CUDA time to free memory
    torch.cuda.empty_cache()
    torch.cuda.synchronize()
    print("[Guilder] Cleanup complete")

    print("========== Stage 2: Solver ==========")
    solver_messages = []
    for q, g in zip(questions, all_guidelines):
        user_content = (
            f"QUESTION:\n{q}\n\n"
            f"GUIDELINE:\n{g}\n\n"
            "Please strictly follow the GUIDELINE to solve the QUESTION. "
            "Remember to put your reasoning inside <think> </think> and final answer inside \\boxed{}."
        )
        solver_messages.append([
            {"role": "system", "content": SOLVER_SYSTEM_PROMPT},
            {"role": "user", "content": user_content},
        ])

    # Solver model should be base model, so no tokenizer override needed
    solver_llm = LLM(
        model=args.solver_model,
        tensor_parallel_size=args.tensor_parallel_size,
        trust_remote_code=True,
        gpu_memory_utilization=args.gpu_memory_utilization,
        enforce_eager=True,  # Disable CUDA graphs to avoid compilation errors
        max_num_seqs=128,  # Limit concurrent sequences to reduce memory
        max_model_len=8192,  # Reduce max sequence length to save memory
    )

    solver_params = SamplingParams(
        temperature=args.solver_temperature,
        top_p=args.solver_top_p,
        max_tokens=args.solver_max_new_tokens,
    )

    print("[Solver] Generating answers...")
    all_outputs: List[str] = []
    for start in range(0, len(solver_messages), args.solver_batch_size):
        end = min(start + args.solver_batch_size, len(solver_messages))
        batch = solver_messages[start:end]
        outputs = solver_llm.chat(messages=batch, sampling_params=solver_params, use_tqdm=False)
        for out in outputs:
            all_outputs.append(out.outputs[0].text.strip())

    pred_rows = []
    for idx, (q, g, gold_raw, pred_raw) in enumerate(zip(questions, all_guidelines, gold_answers, all_outputs)):
        # Parse answers based on dataset
        if args.dataset == "gsm8k":
            gold_norm = parse_gsm8k_answer(gold_raw)
            pred_norm = parse_boxed_answer(pred_raw)
        elif args.dataset == "math":
            # MATH uses \boxed{} in solution, extract it
            gold_norm = parse_boxed_answer(gold_raw)
            pred_norm = parse_boxed_answer(pred_raw)
        elif args.dataset == "commonsenseqa":
            # CommonSenseQA: answerKey is like "A", "B", etc.
            gold_norm = str(gold_raw).strip().upper()
            # First try to extract from \boxed{} (preferred format)
            boxed_match = re.search(r'\\boxed\{([^}]*)\}', pred_raw)
            if boxed_match:
                boxed_content = boxed_match.group(1).strip().upper()
                # Check if it's a valid choice letter (A-E)
                if boxed_content in ['A', 'B', 'C', 'D', 'E']:
                    pred_norm = boxed_content
                else:
                    # If boxed content is not a letter, try to extract letter from it
                    letter_match = re.search(r'\b([A-E])\b', boxed_content, re.IGNORECASE)
                    pred_norm = letter_match.group(1).upper() if letter_match else ""
            else:
                # Fallback: Extract choice from prediction (look for A, B, C, D, E)
                m = re.search(r'\b([A-E])\b', pred_raw, re.IGNORECASE)
                pred_norm = m.group(1).upper() if m else ""
        elif args.dataset == "strategyqa":
            # StrategyQA: answer is True/False
            gold_norm = str(gold_raw).strip().lower()
            # First try to extract from \boxed{} (preferred format)
            boxed_match = re.search(r'\\boxed\{([^}]*)\}', pred_raw)
            if boxed_match:
                boxed_content = boxed_match.group(1).strip().lower()
                # Check if it's yes/no/true/false
                if "yes" in boxed_content or "true" in boxed_content:
                    pred_norm = "true"
                elif "no" in boxed_content or "false" in boxed_content:
                    pred_norm = "false"
                else:
                    pred_norm = ""
            else:
                # Fallback: Extract yes/no from prediction
                pred_lower = pred_raw.lower()
                if "yes" in pred_lower or "true" in pred_lower:
                    pred_norm = "true"
                elif "no" in pred_lower or "false" in pred_lower:
                    pred_norm = "false"
                else:
                    pred_norm = ""
        pred_rows.append({
            "index": idx,
            "question": q,
            "guideline": g,
            "gold_answer_raw": gold_raw,
            "pred_raw": pred_raw,
            "gold_norm": gold_norm,
            "pred_norm": pred_norm,
        })

    write_jsonl(pred_jsonl, pred_rows)
    print(f"[Solver] Saved predictions to {pred_jsonl}")

    cleanup_model(solver_llm)

    print("========== Evaluation ==========")
    pairs = [(r["gold_norm"], r["pred_norm"]) for r in pred_rows]
    acc = compute_accuracy(pairs)
    print(f"[Eval] #examples = {len(pairs)}")
    print(f"[Eval] Accuracy  = {acc * 100:.2f}%")

    summary_path = os.path.join(args.out_dir, f"{args.dataset}_{args.split}_summary.txt")
    with open(summary_path, "w", encoding="utf-8") as f:
        f.write(f"num_examples: {len(pairs)}\n")
        f.write(f"accuracy: {acc:.6f}\n")

    print(f"[Eval] Summary saved to {summary_path}")


if __name__ == "__main__":
    main()
