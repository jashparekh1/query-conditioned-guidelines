import argparse
import json
import os
from typing import List, Dict, Any

import datasets
from openai import OpenAI

try:
    from tqdm import tqdm  # type: ignore
except Exception:  # pragma: no cover
    def tqdm(x, **kwargs):
        return x


GUIDE_SYSTEM_PROMPT = (
    "You are a reasoning planner that creates structured, step-by-step guidelines for solving a given problem.\n"
    "Your goal is not to answer the question directly, but to produce a high-quality, explicit plan that guides another model to solve it.\n"
    "Each plan should:\n"
    "1. Analyze what the problem is asking.\n"
    "2. Identify the required knowledge, sub-tasks, or reasoning steps.\n"
    "3. Provide a structured outline or set of instructions to follow.\n\n"
    "Format your output as a concise, ordered list of reasoning steps or directives. Avoid giving the final answer."
)


SOLVER_SYSTEM_PROMPT = (
    "You are a careful and disciplined problem solver that follows a given guideline to reason step by step and produce the final answer.\n\n"
    "You are provided with:\n"
    "(1) A QUESTION that needs to be solved.\n"
    "(2) A GUIDELINE generated by another model that describes how to approach the problem.\n\n"
    "Your task:\n"
    "- STRICTLY follow the provided guideline to perform reasoning.\n"
    "- You must first think about the reasoning process as an internal monologue before giving the final answer.\n"
    "- The reasoning process MUST be enclosed within <think> </think> tags.\n"
    "- The final answer MUST be placed inside \\boxed{}.\n"
    "- When performing reasoning, use precise and mathematical logic, not vague explanations.\n"
    "- Do NOT reveal hidden reasoning outside of the <think> block.\n"
    "- If the guideline has gaps, fill them in coherently without deviating from its intent.\n\n"
    "Format:\n"
    "<think>\nYour detailed internal reasoning process, including any calculations, logic, or derivations.\n</think>\n"
    "\\boxed{your final answer here}"
)


def extract_solution(solution_str: str) -> str:
    """Extract final numeric solution after '#### ' in GSM8K's reference answer."""
    import re

    pattern = re.search(r"#### (\-?[0-9\.\,]+)", solution_str)
    if pattern is None:
        return ""
    final_solution = pattern.group(0).split("#### ")[1].replace(",", "")
    return final_solution


def grade_answer(pred: str, gt: str) -> bool:
    """Return True if pred matches gt numerically (loose)."""
    import re

    # Prefer boxed extraction
    boxed = re.search(r"\\boxed\{([^}]*)\}", pred)
    if boxed:
        candidate = boxed.group(1).strip()
    else:
        candidate = pred.strip()

    # Normalize numbers
    def norm(x: str) -> str:
        return x.replace(",", "").strip()

    return norm(candidate) == norm(gt)


def build_openai_client(base_url: str) -> OpenAI:
    return OpenAI(base_url=base_url, api_key=os.environ.get("OPENAI_API_KEY", "EMPTY"))


def chat_complete(client: OpenAI, model: str, messages: List[Dict[str, Any]], temperature: float = 0.0, max_tokens: int = 1024) -> str:
    completion = client.chat.completions.create(
        model=model,
        messages=messages,
        temperature=temperature,
        max_tokens=max_tokens,
    )
    return completion.choices[0].message.content


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--dataset_path", default=os.path.expanduser("~/data/gsm8k"))
    parser.add_argument("--split", default="test", choices=["train", "test"])  # evaluate test by default
    parser.add_argument("--limit", type=int, default=None)
    parser.add_argument("--guide_base_url", default="http://127.0.0.1:1226/v1")
    parser.add_argument("--guide_model_name", default="guide-llm")
    parser.add_argument("--solver_base_url", default="http://127.0.0.1:1225/v1")
    parser.add_argument("--solver_model_name", default="base-llm")
    parser.add_argument("--output_path", default="gsm8k_guideline_eval_results.jsonl")
    parser.add_argument("--max_items", type=int, default=None)
    args = parser.parse_args()

    # Load dataset parquet/json produced by examples/data_preprocess/guidelines.py or gsm8k.py
    if os.path.isdir(args.dataset_path):
        # Try parquet first
        parquet_train = os.path.join(args.dataset_path, "train.parquet")
        parquet_test = os.path.join(args.dataset_path, "test.parquet")
        json_mix = os.path.join(args.dataset_path, "dataset.json")
        if os.path.exists(json_mix):
            with open(json_mix, "r", encoding="utf-8") as f:
                bundle = json.load(f)
            data = bundle[args.split]
            dataset = datasets.Dataset.from_list(data)
        elif os.path.exists(parquet_train) and os.path.exists(parquet_test):
            dataset_all = datasets.load_dataset("parquet", data_files={
                "train": parquet_train,
                "test": parquet_test,
            })
            dataset = dataset_all[args.split]
        else:
            # Fallback to HF gsm8k if path points to HF repo
            dataset_all = datasets.load_dataset(args.dataset_path, "main")
            dataset = dataset_all[args.split]
    else:
        dataset_all = datasets.load_dataset(args.dataset_path, "main")
        dataset = dataset_all[args.split]

    if args.limit is not None:
        dataset = dataset.select(range(min(args.limit, len(dataset))))

    guide_client = build_openai_client(args.guide_base_url)
    solver_client = build_openai_client(args.solver_base_url)

    total = 0
    correct = 0

    with open(args.output_path, "w", encoding="utf-8") as fout:
        for ex in tqdm(dataset, total=len(dataset), desc="GSM8K guideline eval"):
            # examples/data_preprocess/guidelines.py made items with keys: prompt(messages), reward_model.ground_truth, extra_info.question/answer
            if "prompt" in ex:
                # prompt is list of messages containing SYSTEM-like instructions + question
                # We extract user question from extra_info.question if present
                question = ex.get("extra_info", {}).get("question")
                if not question:
                    # try parse from prompt
                    try:
                        user_msgs = [m for m in ex["prompt"] if m.get("role") == "user"]
                        if user_msgs:
                            question = user_msgs[-1]["content"]
                        else:
                            question = ""
                    except Exception:
                        question = ""
                gt = ex.get("reward_model", {}).get("ground_truth")
            else:
                # raw gsm8k format
                question = ex.get("question", "")
                answer_raw = ex.get("answer", "")
                gt = extract_solution(answer_raw)

            total += 1

            # 1) generate guideline
            guide_messages = [
                {"role": "system", "content": GUIDE_SYSTEM_PROMPT},
                {"role": "user", "content": f"Question: {question}"},
            ]
            guideline = chat_complete(
                guide_client, args.guide_model_name, guide_messages, temperature=0.2, max_tokens=1024
            )

            # 2) use guideline to solve with base model
            solver_messages = [
                {"role": "system", "content": SOLVER_SYSTEM_PROMPT},
                {"role": "user", "content": f"Question: {question}\n\nGuideline: {guideline}"},
            ]
            solution = chat_complete(
                solver_client, args.solver_model_name, solver_messages, temperature=0.0, max_tokens=1024
            )

            is_correct = grade_answer(solution, gt)
            if is_correct:
                correct += 1

            rec = {
                "question": question,
                "guideline": guideline,
                "solution": solution,
                "ground_truth": gt,
                "correct": bool(is_correct),
            }
            fout.write(json.dumps(rec, ensure_ascii=False) + "\n")

    acc = correct / max(1, total)
    print(json.dumps({"total": total, "correct": correct, "accuracy": acc}, ensure_ascii=False))


if __name__ == "__main__":
    main()


