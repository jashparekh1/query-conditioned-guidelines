torch>=2.0.0
transformers>=4.40.0
accelerate>=0.20.0
bitsandbytes>=0.41.0
safetensors>=0.3.0
pandas>=1.5.0
pyarrow>=10.0.0

# Optional: Flash Attention for better sliding window attention support
# Install with: pip install flash-attn --no-build-isolation
# flash-attn>=2.0.0
